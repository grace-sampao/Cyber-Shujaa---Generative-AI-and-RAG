{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21208c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline\n",
    "from langchain_classic.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afdbd7",
   "metadata": {},
   "source": [
    "### 1. Upload the `document.pdf` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e476096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document loaded successfully!\n",
      "Total pages/documents loaded: 15\n",
      "--------------------------------------------------\n",
      "Snippet of Page 01 Content (First 200 chars):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "--------------------------------------------------\n",
      "Metadata of Page 01: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'document.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Define the file path:\n",
    "PDF_FILE_PATH = \"document.pdf\"\n",
    "\n",
    "# Initialize PyPDFLoader with the file path:\n",
    "loader = PyPDFLoader(PDF_FILE_PATH)\n",
    "\n",
    "# Load the document content:\n",
    "documents = loader.load()\n",
    "\n",
    "# Print summary if successful:\n",
    "print(f\"‚úÖ Document loaded successfully!\")\n",
    "print(f\"Total pages/documents loaded: {len(documents)}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Snippet of Page 01 Content (First 200 chars):\")\n",
    "print(documents[0].page_content[:200] + \"...\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Metadata of Page 01: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4242b",
   "metadata": {},
   "source": [
    "### 2. Split the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00363663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document successfully split into chunks!\n",
      "Original number of pages/documents: 15\n",
      "Total number of chunks created: 52\n",
      "Example Chunk Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'document.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "Example Chunk Content (Length: 986):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par...\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the Text Splitter:\n",
    "CHUNK_SIZE = 1000     # No. of characters in each chunk\n",
    "CHUNK_OVERLAP = 200   # No. of overlapping characters between adjacent chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "# 2. Split the documents:\n",
    "chunked_documents = text_splitter.split_documents(documents)    # processes the list of large documents into a list of smaller ones\n",
    "\n",
    "# Verification:\n",
    "print(f\"‚úÖ Document successfully split into chunks!\")\n",
    "print(f\"Original number of pages/documents: {len(documents)}\")\n",
    "print(f\"Total number of chunks created: {len(chunked_documents)}\")\n",
    "# Show that metadata is preserved (e.g., page number):\n",
    "print(f\"Example Chunk Metadata: {chunked_documents[0].metadata}\")\n",
    "print(f\"Example Chunk Content (Length: {len(chunked_documents[0].page_content)}):\")\n",
    "print(chunked_documents[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38174208",
   "metadata": {},
   "source": [
    "### 3. Create embeddings in the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c597a76",
   "metadata": {},
   "source": [
    "This is where the document's raw text is transformed into a searchable knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c87fed0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model 'sentence-transformers/all-MiniLM-L6-v2' loaded successfully.\n",
      "‚úÖ FAISS Vector Store created with 52 documents.\n",
      "--------------------------------------------------\n",
      "Index saved locally to folder: faiss_index_rag\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Embedding Model:\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"   # good, lightweight, open-source sentence-transformer model\n",
    "\n",
    "# Load the embedding model:\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "print(f\"‚úÖ Embedding model '{EMBEDDING_MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "# Create the FAISS Vector Store:\n",
    "  # FAISS.from_documents handles the following automatically:\n",
    "    # Taking each Document (chunk)\n",
    "    # Generating its embedding using the 'embeddings' model\n",
    "    # Storing the resulting vector and the original text/metadata in the FAISS database\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ FAISS Vector Store created with {len(chunked_documents)} documents.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Save the FAISS index locally, to avoid re-running embedding every time:\n",
    "FAISS_INDEX_PATH = \"faiss_index_rag\"\n",
    "vector_store.save_local(FAISS_INDEX_PATH)\n",
    "print(f\"Index saved locally to folder: {FAISS_INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b2feb",
   "metadata": {},
   "source": [
    "### 4. Add LLM of choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9853b",
   "metadata": {},
   "source": [
    "Choose a model optimized for question answering and text generation, often built \n",
    "on a **Sequence-to-Sequence (Seq2Seq)** architecture like **T5** or **BART**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128ad78",
   "metadata": {},
   "source": [
    "Use the **Flan-T5** model‚éØa powerful, relatively small, and effective LLM for \n",
    "this type of task.\n",
    "\n",
    "Requires the `AutoTokenizer` and `AutoModelForSeq2SeqLM` imports, along with the \n",
    "LangChain utility `RetrievalQA` to combine the retriever and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef240a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generative LLM 'google/flan-t5-base' loaded and configured.\n",
      "‚úÖ RAG Chain (RetrievalQA) successfully constructed.\n",
      "--------------------------------------------------\n",
      "‚ùì Querying the RAG system: What is the primary motivation for using the attention mechanism \n",
      "instead of recurrence and convolution in the Transformer model?\n",
      "\n",
      "ü§ñ RAG Answer:\n",
      "the amount of computation that can be parallelized\n",
      "\n",
      "üìú Source Document Metadata (to verify retrieval):\n",
      "- Page: 5, Source: document.pdf\n",
      "- Page: 2, Source: document.pdf\n",
      "- Page: 6, Source: document.pdf\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Generative LLM (Flan-T5):\n",
    "LLM_MODEL_NAME = \"google/flan-t5-base\"    # good balance of size and performance\n",
    "\n",
    "# Load Tokenizer and Model:\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_NAME)\n",
    "\n",
    "# Create the HuggingFace Pipepline for Text-to-Text Generation:\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,     # max length for the generated answer\n",
    "    temperature=0.1,    # lower temperature for factual answers\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Wrap the Pipeline in a LangChain LLM object:\n",
    "  # Allows the LLM to be easily used in the LangChain framework.\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(f\"‚úÖ Generative LLM '{LLM_MODEL_NAME}' loaded and configured.\")\n",
    "\n",
    "# Connect the Retriever to the LLM (Building the RAG Chain):\n",
    "  # Create the Retriever from the FAISS Vector Score.\n",
    "    # The retriever component knows how to perform the vector search.\n",
    "    # Ask it to return the top 3 most relevant chunks (k=3)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Build the RAG Chain (RetrievalQA):\n",
    "\t# Automatically handles:\n",
    "\t\t# a) Taking the user query\n",
    "    # b) Calling the 'retriever' to get relevant chunks\n",
    "    # c) Formatting the chunks and the query into a single prompt\n",
    "    # d) Feeding the prompt to the 'llm' for final answer generation\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\t\t# stuffs all retrieved chunks into the prompt\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\t\t# for verification later\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG Chain (RetrievalQA) successfully constructed.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test the Pipeline (Initial Query):\n",
    "\t# Use a question specific to document.pdf.\n",
    "RAG_QUERY = '''What is the primary motivation for using the attention mechanism \n",
    "instead of recurrence and convolution in the Transformer model?'''\n",
    "\n",
    "print(f\"‚ùì Querying the RAG system: {RAG_QUERY}\")\n",
    "result = qa_chain.invoke({\"query\": RAG_QUERY})\n",
    "\n",
    "print(\"\\nü§ñ RAG Answer:\")\n",
    "print(result['result'])\n",
    "print(\"\\nüìú Source Document Metadata (to verify retrieval):\")\n",
    "for document in result['source_documents']:\n",
    "    print(f\"- Page: {document.metadata.get('page', 'N/A') + 1}, Source: {document.metadata.get('source', 'document.pdf')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819248f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative_ai_and_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
