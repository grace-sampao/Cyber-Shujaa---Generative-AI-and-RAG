{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21208c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadupoy/Documents/Programming/Data Science & AI/Cyber Shujaa/Generative AI and RAG/generative_ai_and_rag/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "/Users/nadupoy/Documents/Programming/Data Science & AI/Cyber Shujaa/Generative AI and RAG/generative_ai_and_rag/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline\n",
    "from langchain_classic.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afdbd7",
   "metadata": {},
   "source": [
    "### 1. Upload the `document.pdf` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e476096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document loaded successfully!\n",
      "Total pages/documents loaded: 15\n",
      "--------------------------------------------------\n",
      "Snippet of Page 01 Content (First 200 chars):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "--------------------------------------------------\n",
      "Metadata of Page 01: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'document.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Define the file path:\n",
    "PDF_FILE_PATH = \"document.pdf\"\n",
    "\n",
    "# Initialize PyPDFLoader with the file path:\n",
    "loader = PyPDFLoader(PDF_FILE_PATH)\n",
    "\n",
    "# Load the document content:\n",
    "documents = loader.load()\n",
    "\n",
    "# Print summary if successful:\n",
    "print(f\"‚úÖ Document loaded successfully!\")\n",
    "print(f\"Total pages/documents loaded: {len(documents)}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Snippet of Page 01 Content (First 200 chars):\")\n",
    "print(documents[0].page_content[:200] + \"...\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Metadata of Page 01: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4242b",
   "metadata": {},
   "source": [
    "### 2. Split the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00363663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document successfully split into chunks!\n",
      "Original number of pages/documents: 15\n",
      "Total number of chunks created: 52\n",
      "Example Chunk Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'document.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "Example Chunk Content (Length: 986):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par...\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the Text Splitter:\n",
    "CHUNK_SIZE = 1000     # No. of characters in each chunk\n",
    "CHUNK_OVERLAP = 200   # No. of overlapping characters between adjacent chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "\tchunk_size=CHUNK_SIZE,\n",
    "\tchunk_overlap=CHUNK_OVERLAP,\n",
    "\tlength_function=len,\n",
    "\tis_separator_regex=False\n",
    ")\n",
    "\n",
    "# 2. Split the documents:\n",
    "chunked_documents = text_splitter.split_documents(documents)    # processes the list of large documents into a list of smaller ones\n",
    "\n",
    "# Verification:\n",
    "print(f\"‚úÖ Document successfully split into chunks!\")\n",
    "print(f\"Original number of pages/documents: {len(documents)}\")\n",
    "print(f\"Total number of chunks created: {len(chunked_documents)}\")\n",
    "# Show that metadata is preserved (e.g., page number):\n",
    "print(f\"Example Chunk Metadata: {chunked_documents[0].metadata}\")\n",
    "print(f\"Example Chunk Content (Length: {len(chunked_documents[0].page_content)}):\")\n",
    "print(chunked_documents[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38174208",
   "metadata": {},
   "source": [
    "### 3. Create embeddings in the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c597a76",
   "metadata": {},
   "source": [
    "This is where the document's raw text is transformed into a searchable knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87fed0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model 'sentence-transformers/all-MiniLM-L6-v2' loaded successfully.\n",
      "‚úÖ FAISS Vector Store created with 52 documents.\n",
      "--------------------------------------------------\n",
      "Index saved locally to folder: faiss_index_rag\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Embedding Model:\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"   # good, lightweight, open-source sentence-transformer model\n",
    "\n",
    "# Load the embedding model:\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "print(f\"‚úÖ Embedding model '{EMBEDDING_MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "# Create the FAISS Vector Store:\n",
    "\t# FAISS.from_documents handles the following automatically:\n",
    "\t\t# Taking each Document (chunk)\n",
    "\t\t# Generating its embedding using the 'embeddings' model\n",
    "\t\t# Storing the resulting vector and the original text/metadata in the FAISS database\n",
    "vector_store = FAISS.from_documents(\n",
    "\tdocuments=chunked_documents,\n",
    "\tembedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ FAISS Vector Store created with {len(chunked_documents)} documents.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Save the FAISS index locally, to avoid re-running embedding every time:\n",
    "FAISS_INDEX_PATH = \"faiss_index_rag\"\n",
    "vector_store.save_local(FAISS_INDEX_PATH)\n",
    "print(f\"Index saved locally to folder: {FAISS_INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b2feb",
   "metadata": {},
   "source": [
    "### 4. Add LLM of choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9853b",
   "metadata": {},
   "source": [
    "Choose a model optimized for question answering and text generation, often built \n",
    "on a **Sequence-to-Sequence (Seq2Seq)** architecture like **T5** or **BART**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128ad78",
   "metadata": {},
   "source": [
    "Use the **Flan-T5** model‚éØa powerful, relatively small, and effective LLM for \n",
    "this type of task.\n",
    "\n",
    "Requires the `AutoTokenizer` and `AutoModelForSeq2SeqLM` imports, along with the \n",
    "LangChain utility `RetrievalQA` to combine the retriever and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef240a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generative LLM 'google/flan-t5-base' loaded and configured.\n",
      "‚úÖ RAG Chain (RetrievalQA) successfully constructed.\n",
      "--------------------------------------------------\n",
      "‚ùì Querying the RAG system: What is the primary motivation for using the attention mechanism \n",
      "instead of recurrence and convolution in the Transformer model?\n",
      "\n",
      "ü§ñ RAG Answer:\n",
      "the amount of computation that can be parallelized\n",
      "\n",
      "üìú Source Document Metadata (to verify retrieval):\n",
      "- Page: 5, Source: document.pdf\n",
      "- Page: 2, Source: document.pdf\n",
      "- Page: 6, Source: document.pdf\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Generative LLM (Flan-T5):\n",
    "LLM_MODEL_NAME = \"google/flan-t5-base\"    # good balance of size and performance\n",
    "\n",
    "# Load Tokenizer and Model:\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_NAME)\n",
    "\n",
    "# Create the HuggingFace Pipepline for Text-to-Text Generation:\n",
    "pipe = pipeline(\n",
    "\t\"text2text-generation\",\n",
    "\tmodel=model,\n",
    "\ttokenizer=tokenizer,\n",
    "\tmax_length=512,     # max length for the generated answer\n",
    "\ttemperature=0.1,    # lower temperature for factual answers\n",
    "\tdo_sample=True\n",
    ")\n",
    "\n",
    "# Wrap the Pipeline in a LangChain LLM object:\n",
    "\t# Allows the LLM to be easily used in the LangChain framework.\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(f\"‚úÖ Generative LLM '{LLM_MODEL_NAME}' loaded and configured.\")\n",
    "\n",
    "# Connect the Retriever to the LLM (Building the RAG Chain):\n",
    "\t# Create the Retriever from the FAISS Vector Score.\n",
    "\t\t# The retriever component knows how to perform the vector search.\n",
    "\t\t# Ask it to return the top 3 most relevant chunks (k=3)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Build the RAG Chain (RetrievalQA):\n",
    "\t# Automatically handles:\n",
    "\t\t# a) Taking the user query\n",
    "\t\t# b) Calling the 'retriever' to get relevant chunks\n",
    "\t\t# c) Formatting the chunks and the query into a single prompt\n",
    "\t\t# d) Feeding the prompt to the 'llm' for final answer generation\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "\tllm=llm,\n",
    "\tchain_type=\"stuff\",\t\t# stuffs all retrieved chunks into the prompt\n",
    "\tretriever=retriever,\n",
    "\treturn_source_documents=True\t\t# for verification later\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG Chain (RetrievalQA) successfully constructed.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test the Pipeline (Initial Query):\n",
    "\t# Use a question specific to document.pdf.\n",
    "RAG_QUERY = '''What is the primary motivation for using the attention mechanism \n",
    "instead of recurrence and convolution in the Transformer model?'''\n",
    "\n",
    "print(f\"‚ùì Querying the RAG system: {RAG_QUERY}\")\n",
    "result = qa_chain.invoke({\"query\": RAG_QUERY})\n",
    "\n",
    "print(\"\\nü§ñ RAG Answer:\")\n",
    "print(result['result'])\n",
    "print(\"\\nüìú Source Document Metadata (to verify retrieval):\")\n",
    "for document in result['source_documents']:\n",
    "\t\tprint(f\"- Page: {document.metadata.get('page', 'N/A') + 1}, Source: {document.metadata.get('source', 'document.pdf')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819248f3",
   "metadata": {},
   "source": [
    "### 5. Query using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53451718",
   "metadata": {},
   "source": [
    "- Demonstrate how retrieval improves generative question-answering by comparing answers generated from document-grounded context versus generic answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e88d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üéØ Query for Comparison: In the Transformer paper, how many layers are used in the Encoder and Decoder stacks, and what is the output dimensionality of the layers?\n",
      "==================================================\n",
      "\n",
      "ü§ñ 1. RAG Answer (Document-Grounded Context)\n",
      "N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "--------------------------------------------------\n",
      "Source Page(s) Verified: [6, 6, 6]\n",
      "--------------------------------------------------\n",
      "\n",
      "üß† 2. Generic LLM Answer (Ungrounded, No Context)\n",
      "four\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Define the Query ---\n",
    "\t# Choose a highly specific question only answerable by the document content.\n",
    "SPECIFIC_QUERY = \"In the Transformer paper, how many layers are used in the Encoder and Decoder stacks, and what is the output dimensionality of the layers?\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"üéØ Query for Comparison: {SPECIFIC_QUERY}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# A. Comparison 1: RAG (Grounded) vs. Generic (Ungrounded)\n",
    "\t# 1. RAG Answer (Document-Grounded):\n",
    "print(\"\\nü§ñ 1. RAG Answer (Document-Grounded Context)\")\n",
    "rag_result = qa_chain.invoke({\"query\": SPECIFIC_QUERY})\n",
    "rag_answer = rag_result['result']\n",
    "print(rag_answer)\n",
    "print(\"-\" * 50)\n",
    "print(f\"Source Page(s) Verified: {[document.metadata.get('page') + 1 for doc in rag_result['source_documents']]}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\t# 2. Generic LLM Answer (Ungrounded/General Knowledge):\n",
    "print(\"\\nüß† 2. Generic LLM Answer (Ungrounded, No Context)\")\n",
    "\t\t# Manually add an instruction prompt to the base LLM for the best comparison.\n",
    "generic_prompt = f\"Answer the following question clearly and concisely. If you don't know the answer, say you can't find it. Question: {SPECIFIC_QUERY}\"\n",
    "generic_answer = llm.invoke(generic_prompt)\n",
    "print(generic_answer)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d471a4",
   "metadata": {},
   "source": [
    "- Practice prompt engineering to structure queries that guide the generative model for clearer and more detailed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è 3. Prompt Engineered RAG Answer\n",
      "N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
      "--------------------------------------------------\n",
      "Source Page(s) Verified: [6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "\t# 3. Prompt Engineered RAG Query:\n",
    "\t\t# Practice prompt engineering to demand a specific format or level of detail.\n",
    "PROMPT_ENGINEERED_QUERY = \"\"\"\n",
    "Using ONLY the provided context, answer the following question.\n",
    "Structure your answer into two clear sentences.\n",
    "First Sentence: State the number of layers in the Encoder and Decoder stacks.\n",
    "Second Sentence: Specify the output dimensionality, d_model.\n",
    "Question: How many layers are used in the Encoder and Decoder stacks, and what is the output dimensionality of the layers?\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n‚öôÔ∏è 3. Prompt Engineered RAG Answer\")\n",
    "engineered_result = qa_chain.invoke({\"query\": PROMPT_ENGINEERED_QUERY})\n",
    "engineered_answer = engineered_result['result']\n",
    "print(engineered_answer)\n",
    "print(\"-\" * 50)\n",
    "print(f\"Source Page(s) Verified: {[document.metadata.get('page') + 1 for doc in engineered_result['source_documents']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3c5b9",
   "metadata": {},
   "source": [
    "### üêç RAGPipeline Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b0dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff81538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG Pipeline Components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FAISS Index from 'faiss_index_rag'...\n",
      "‚úÖ Index loaded successfully.\n",
      "‚ú® RAG Pipeline is ready to query.\n",
      "\n",
      "‚ùì Query: What is the primary motivation for using the attention mechanism in the Transformer model?\n",
      "\n",
      "ü§ñ Answer:\n",
      "to jointly attend to information from different representation subspaces at different positions\n",
      "\n",
      "üìú Source Document Pages:\n",
      "Source Pages: [6]\n",
      "\n",
      "‚ùì Query: Describe the input and output layers of the Transformer architecture.\n",
      "\n",
      "ü§ñ Answer:\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. The Transformer uses multi-head attention in three different ways: ‚Ä¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. ‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n",
      "\n",
      "üìú Source Document Pages:\n",
      "Source Pages: [6]\n"
     ]
    }
   ],
   "source": [
    "class RAGPipeline:\n",
    "\tdef __init__(self):\n",
    "\t\t# Initializes components and checks for existing index.\n",
    "\t\tprint(\"Initializing RAG Pipeline Components...\")\n",
    "\t\t\t\t\n",
    "\t\t# 1. Initialize Embeddings Model (Required for both creation and loading):\n",
    "\t\tself.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\t\t\t\t\n",
    "\t\t# 2. Initialize LLM:\n",
    "\t\ttokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "\t\tmodel = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_NAME)\n",
    "\t\tpipe = pipeline(\n",
    "\t\t\t\"text2text-generation\",\n",
    "\t\t\tmodel=model,\n",
    "\t\t\ttokenizer=tokenizer,\n",
    "\t\t\tmax_length=512,\n",
    "\t\t\ttemperature=0.1,\n",
    "\t\t\tdo_sample=True\n",
    "\t\t\t\t)\n",
    "\t\tself.llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\t\tself.vector_store = None\n",
    "\t\tself.qa_chain = None\n",
    "\t\t\t\t\n",
    "\tdef _create_index(self):\n",
    "\t\t\"\"\"Loads PDF, splits it, creates embeddings, and saves FAISS index.\"\"\"\n",
    "\t\tprint(f\"Index not found. Creating index from {PDF_FILE_PATH}...\")\n",
    "\t\t\n",
    "\t\t# 1. Load Document\n",
    "\t\tloader = PyPDFLoader(PDF_FILE_PATH)\n",
    "\t\tdocuments = loader.load()\n",
    "\t\t\n",
    "\t\t# 2. Split Document\n",
    "\t\ttext_splitter = RecursiveCharacterTextSplitter(\n",
    "\t\t\t\tchunk_size=CHUNK_SIZE,\n",
    "\t\t\t\tchunk_overlap=CHUNK_OVERLAP\n",
    "\t\t)\n",
    "\t\tchunked_documents = text_splitter.split_documents(documents)\n",
    "\t\tprint(f\"Documents split into {len(chunked_documents)} chunks.\")\n",
    "\t\t\n",
    "\t\t# 3. Create FAISS Vector Store and Index\n",
    "\t\tself.vector_store = FAISS.from_documents(\n",
    "\t\t\t\tdocuments=chunked_documents,\n",
    "\t\t\t\tembedding=self.embeddings\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# 4. Save Index to Disk (Persistence)\n",
    "\t\tself.vector_store.save_local(FAISS_INDEX_PATH)\n",
    "\t\tprint(f\"‚úÖ FAISS Index created and saved to '{FAISS_INDEX_PATH}'.\")\n",
    "\t\t\n",
    "\tdef setup_pipeline(self):\n",
    "\t\t\"\"\"Loads index if it exists; otherwise, creates and saves it.\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\t# Check if the FAISS index folder exists\n",
    "\t\t\tif os.path.exists(FAISS_INDEX_PATH) and len(os.listdir(FAISS_INDEX_PATH)) > 0:\n",
    "\t\t\t\tprint(f\"Loading existing FAISS Index from '{FAISS_INDEX_PATH}'...\")\n",
    "\t\t\t\t# The FAISS.load_local function requires the embeddings model\n",
    "\t\t\t\tself.vector_store = FAISS.load_local(\n",
    "\t\t\t\t\tFAISS_INDEX_PATH,\n",
    "\t\t\t\t\tself.embeddings,\n",
    "\t\t\t\t\t# Set this to True to avoid a potential deprecation warning/error in newer versions\n",
    "\t\t\t\t\tallow_dangerous_deserialization=True\n",
    "\t\t\t\t)\n",
    "\t\t\t\tprint(\"‚úÖ Index loaded successfully.\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tself._create_index()\n",
    "\n",
    "\t\t\t# 5. Create the Retrieval Chain (Retriever + LLM)\n",
    "\t\t\tretriever = self.vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\t\t\tself.qa_chain = RetrievalQA.from_chain_type(\n",
    "\t\t\t\tllm=self.llm,\n",
    "\t\t\t\tchain_type=\"stuff\",\n",
    "\t\t\t\tretriever=retriever,\n",
    "\t\t\t\treturn_source_documents=True\n",
    "\t\t\t)\n",
    "\t\t\tprint(\"‚ú® RAG Pipeline is ready to query.\")\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"An error occured during setup: {e}\")\n",
    "\t\n",
    "\tdef query(self, user_query):\n",
    "\t\t\"\"\"Executes a query against the RAG chain.\"\"\"\n",
    "\t\tif not self.qa_chain:\n",
    "\t\t\tprint(\"Error RAG pipeline not set up. Run setup_pipeline() first.\")\n",
    "\t\t\treturn {\n",
    "\t\t\t\t\"resut\": \"Pipeline not initialized.\",\n",
    "\t\t\t\t\"source_documents\": []\n",
    "\t\t\t}\n",
    "\t\t\n",
    "\t\tprint(f\"\\n‚ùì Query: {user_query}\")\n",
    "\t\tresult = self.qa_chain.invoke({\"query\": user_query})\n",
    "\n",
    "\t\t# Format the output for clean display\n",
    "\t\tprint(\"\\nü§ñ Answer:\")\n",
    "\t\tprint(result['result'])\n",
    "\t\tprint(\"\\nüìú Source Document Pages:\")\n",
    "\t\tsource_pages = [\n",
    "\t\t\tdocument.metadata.get('page', 'N/A') + 1 for doc in result['source_documents']\n",
    "\t\t]\n",
    "\t\tprint(f\"Source Pages: {list(set(source_pages))}\")\n",
    "\n",
    "\t\treturn result\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "\trag_system = RAGPipeline()\n",
    "\trag_system.setup_pipeline()\t\t\t\t# Creates the index once, loads it on subsequent runs\n",
    "\n",
    "\t# Query the system:\n",
    "\tquestion = \"What is the primary motivation for using the attention mechanism in the Transformer model?\"\n",
    "\trag_system.query(question)\n",
    "\n",
    "\t# Another question can be asked without re-embedding:\n",
    "\tquestion_02 = \"Describe the input and output layers of the Transformer architecture.\"\n",
    "\trag_system.query(question_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed343ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative_ai_and_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
