{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21208c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline\n",
    "from langchain_classic.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afdbd7",
   "metadata": {},
   "source": [
    "### 1. Upload the `document.pdf` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e476096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document loaded successfully!\n",
      "Total pages/documents loaded: 15\n",
      "--------------------------------------------------\n",
      "Snippet of Page 01 Content (First 200 chars):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "--------------------------------------------------\n",
      "Metadata of Page 01: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'document.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Define the file path:\n",
    "PDF_FILE_PATH = \"document.pdf\"\n",
    "\n",
    "# Initialize PyPDFLoader with the file path:\n",
    "loader = PyPDFLoader(PDF_FILE_PATH)\n",
    "\n",
    "# Load the document content:\n",
    "documents = loader.load()\n",
    "\n",
    "# Print summary if successful:\n",
    "print(f\"‚úÖ Document loaded successfully!\")\n",
    "print(f\"Total pages/documents loaded: {len(documents)}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Snippet of Page 01 Content (First 200 chars):\")\n",
    "print(documents[0].page_content[:200] + \"...\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Metadata of Page 01: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4242b",
   "metadata": {},
   "source": [
    "### 2. Split the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00363663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document successfully split into chunks!\n",
      "Original number of pages/documents: 15\n",
      "Total number of chunks created: 52\n",
      "Example Chunk Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'document.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "Example Chunk Content (Length: 986):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par...\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the Text Splitter:\n",
    "CHUNK_SIZE = 1000     # No. of characters in each chunk\n",
    "CHUNK_OVERLAP = 200   # No. of overlapping characters between adjacent chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "# 2. Split the documents:\n",
    "chunked_documents = text_splitter.split_documents(documents)    # processes the list of large documents into a list of smaller ones\n",
    "\n",
    "# Verification:\n",
    "print(f\"‚úÖ Document successfully split into chunks!\")\n",
    "print(f\"Original number of pages/documents: {len(documents)}\")\n",
    "print(f\"Total number of chunks created: {len(chunked_documents)}\")\n",
    "# Show that metadata is preserved (e.g., page number):\n",
    "print(f\"Example Chunk Metadata: {chunked_documents[0].metadata}\")\n",
    "print(f\"Example Chunk Content (Length: {len(chunked_documents[0].page_content)}):\")\n",
    "print(chunked_documents[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38174208",
   "metadata": {},
   "source": [
    "### 3. Create embeddings in the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c597a76",
   "metadata": {},
   "source": [
    "This is where the document's raw text is transformed into a searchable knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c87fed0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model 'sentence-transformers/all-MiniLM-L6-v2' loaded successfully.\n",
      "‚úÖ FAISS Vector Store created with 52 documents.\n",
      "--------------------------------------------------\n",
      "Index saved locally to folder: faiss_index_rag\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Embedding Model:\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"   # good, lightweight, open-source sentence-transformer model\n",
    "\n",
    "# Load the embedding model:\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "print(f\"‚úÖ Embedding model '{EMBEDDING_MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "# Create the FAISS Vector Store:\n",
    "  # FAISS.from_documents handles the following automatically:\n",
    "    # Taking each Document (chunk)\n",
    "    # Generating its embedding using the 'embeddings' model\n",
    "    # Storing the resulting vector and the original text/metadata in the FAISS database\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ FAISS Vector Store created with {len(chunked_documents)} documents.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Save the FAISS index locally, to avoid re-running embedding every time:\n",
    "FAISS_INDEX_PATH = \"faiss_index_rag\"\n",
    "vector_store.save_local(FAISS_INDEX_PATH)\n",
    "print(f\"Index saved locally to folder: {FAISS_INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b2feb",
   "metadata": {},
   "source": [
    "### 4. Add LLM of choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9853b",
   "metadata": {},
   "source": [
    "Choose a model optimized for question answering and text generation, often built \n",
    "on a **Sequence-to-Sequence (Seq2Seq)** architecture like **T5** or **BART**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128ad78",
   "metadata": {},
   "source": [
    "Use the **Flan-T5** model‚éØa powerful, relatively small, and effective LLM for \n",
    "this type of task.\n",
    "\n",
    "Requires the `AutoTokenizer` and `AutoModelForSeq2SeqLM` imports, along with the \n",
    "LangChain utility `RetrievalQA` to combine the retriever and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef240a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generative LLM 'google/flan-t5-base' loaded and configured.\n",
      "‚úÖ RAG Chain (RetrievalQA) successfully constructed.\n",
      "--------------------------------------------------\n",
      "‚ùì Querying the RAG system: What is the primary motivation for using the attention mechanism \n",
      "instead of recurrence and convolution in the Transformer model?\n",
      "\n",
      "ü§ñ RAG Answer:\n",
      "the amount of computation that can be parallelized\n",
      "\n",
      "üìú Source Document Metadata (to verify retrieval):\n",
      "- Page: 5, Source: document.pdf\n",
      "- Page: 2, Source: document.pdf\n",
      "- Page: 6, Source: document.pdf\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Generative LLM (Flan-T5):\n",
    "LLM_MODEL_NAME = \"google/flan-t5-base\"    # good balance of size and performance\n",
    "\n",
    "# Load Tokenizer and Model:\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_NAME)\n",
    "\n",
    "# Create the HuggingFace Pipepline for Text-to-Text Generation:\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,     # max length for the generated answer\n",
    "    temperature=0.1,    # lower temperature for factual answers\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Wrap the Pipeline in a LangChain LLM object:\n",
    "  # Allows the LLM to be easily used in the LangChain framework.\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(f\"‚úÖ Generative LLM '{LLM_MODEL_NAME}' loaded and configured.\")\n",
    "\n",
    "# Connect the Retriever to the LLM (Building the RAG Chain):\n",
    "  # Create the Retriever from the FAISS Vector Score.\n",
    "    # The retriever component knows how to perform the vector search.\n",
    "    # Ask it to return the top 3 most relevant chunks (k=3)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Build the RAG Chain (RetrievalQA):\n",
    "\t# Automatically handles:\n",
    "\t\t# a) Taking the user query\n",
    "    # b) Calling the 'retriever' to get relevant chunks\n",
    "    # c) Formatting the chunks and the query into a single prompt\n",
    "    # d) Feeding the prompt to the 'llm' for final answer generation\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\t\t# stuffs all retrieved chunks into the prompt\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\t\t# for verification later\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG Chain (RetrievalQA) successfully constructed.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test the Pipeline (Initial Query):\n",
    "\t# Use a question specific to document.pdf.\n",
    "RAG_QUERY = '''What is the primary motivation for using the attention mechanism \n",
    "instead of recurrence and convolution in the Transformer model?'''\n",
    "\n",
    "print(f\"‚ùì Querying the RAG system: {RAG_QUERY}\")\n",
    "result = qa_chain.invoke({\"query\": RAG_QUERY})\n",
    "\n",
    "print(\"\\nü§ñ RAG Answer:\")\n",
    "print(result['result'])\n",
    "print(\"\\nüìú Source Document Metadata (to verify retrieval):\")\n",
    "for document in result['source_documents']:\n",
    "    print(f\"- Page: {document.metadata.get('page', 'N/A') + 1}, Source: {document.metadata.get('source', 'document.pdf')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819248f3",
   "metadata": {},
   "source": [
    "### 5. Query using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53451718",
   "metadata": {},
   "source": [
    "- Demonstrate how retrieval improves generative question-answering by comparing answers generated from document-grounded context versus generic answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e88d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üéØ Query for Comparison: In the Transformer paper, how many layers are used in the Encoder and Decoder stacks, and what is the output dimensionality of the layers?\n",
      "==================================================\n",
      "\n",
      "ü§ñ 1. RAG Answer (Document-Grounded Context)\n",
      "N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "--------------------------------------------------\n",
      "Source Page(s) Verified: [6, 6, 6]\n",
      "--------------------------------------------------\n",
      "\n",
      "üß† 2. Generic LLM Answer (Ungrounded, No Context)\n",
      "four\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Define the Query ---\n",
    "\t# Choose a highly specific question only answerable by the document content.\n",
    "SPECIFIC_QUERY = \"In the Transformer paper, how many layers are used in the Encoder and Decoder stacks, and what is the output dimensionality of the layers?\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"üéØ Query for Comparison: {SPECIFIC_QUERY}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# A. Comparison 1: RAG (Grounded) vs. Generic (Ungrounded)\n",
    "\t# 1. RAG Answer (Document-Grounded):\n",
    "print(\"\\nü§ñ 1. RAG Answer (Document-Grounded Context)\")\n",
    "rag_result = qa_chain.invoke({\"query\": SPECIFIC_QUERY})\n",
    "rag_answer = rag_result['result']\n",
    "print(rag_answer)\n",
    "print(\"-\" * 50)\n",
    "print(f\"Source Page(s) Verified: {[document.metadata.get('page') + 1 for doc in rag_result['source_documents']]}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\t# 2. Generic LLM Answer (Ungrounded/General Knowledge):\n",
    "print(\"\\nüß† 2. Generic LLM Answer (Ungrounded, No Context)\")\n",
    "\t\t# Manually add an instruction prompt to the base LLM for the best comparison.\n",
    "generic_prompt = f\"Answer the following question clearly and concisely. If you don't know the answer, say you can't find it. Question: {SPECIFIC_QUERY}\"\n",
    "generic_answer = llm.invoke(generic_prompt)\n",
    "print(generic_answer)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d471a4",
   "metadata": {},
   "source": [
    "- Practice prompt engineering to structure queries that guide the generative model for clearer and more detailed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a451224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è 3. Prompt Engineered RAG Answer\n",
      "N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
      "--------------------------------------------------\n",
      "Source Page(s) Verified: [6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "\t# 3. Prompt Engineered RAG Query:\n",
    "\t\t# Practice prompt engineering to demand a specific format or level of detail.\n",
    "PROMPT_ENGINEERED_QUERY = \"\"\"\n",
    "Using ONLY the provided context, answer the following question.\n",
    "Structure your answer into two clear sentences.\n",
    "First Sentence: State the number of layers in the Encoder and Decoder stacks.\n",
    "Second Sentence: Specify the output dimensionality, d_model.\n",
    "Question: How many layers are used in the Encoder and Decoder stacks, and what is the output dimensionality of the layers?\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n‚öôÔ∏è 3. Prompt Engineered RAG Answer\")\n",
    "engineered_result = qa_chain.invoke({\"query\": PROMPT_ENGINEERED_QUERY})\n",
    "engineered_answer = engineered_result['result']\n",
    "print(engineered_answer)\n",
    "print(\"-\" * 50)\n",
    "print(f\"Source Page(s) Verified: {[document.metadata.get('page') + 1 for doc in engineered_result['source_documents']]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative_ai_and_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
